{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "importing libraries and initialising the variables and values\n",
        "'''\n",
        "# importing libraries\n",
        "import numpy as np\n",
        "#import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import random\n",
        "import glob  # to find files\n",
        "import seaborn as sns\n",
        "# Seaborn library for bar chart\n",
        "#import seaborn as sns\n",
        "\n",
        "# Libraries for TensorFlow\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras import models, layers\n",
        "\n",
        "# Library for Transfer Learning\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "\n",
        "print(\"Importing libraries completed.\")\n",
        "\n",
        "\n",
        "\n",
        "train_folder =\"Data/\"\n",
        "# variables for image size\n",
        "img_width = 100\n",
        "img_height = 100\n",
        "\n",
        "# variable for model\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "print(\"Variable declaration completed.\")\n",
        "\n",
        "# listing the folders containing images\n",
        "\n",
        "# Train Dataset\n",
        "train_class_names = os.listdir(train_folder)\n",
        "print(\"Train class names: %s\" % (train_class_names))\n",
        "# print(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nDataset class name listing completed.\")\n",
        "\n",
        "# declaration of functions\n",
        "\n",
        "\n",
        "# Declaring variables\n",
        "x = []  # to store array value of the images\n",
        "y = []  # to store the labels of the images\n",
        "\n",
        "for folder in os.listdir(train_folder):\n",
        "    image_list = os.listdir(train_folder + \"/\" + folder)\n",
        "    for img_name in image_list:\n",
        "        # Loading images\n",
        "        img = image.load_img(train_folder + \"/\" + folder + \"/\" + img_name, target_size=(img_width, img_height))\n",
        "\n",
        "        # Converting to arrary\n",
        "        img = image.img_to_array(img)\n",
        "\n",
        "        # Transfer Learning: this is to apply preprocess of VGG16 model to our images before passing it to VGG16\n",
        "        img = preprocess_input(img)  # Optional step\n",
        "\n",
        "        # Appending the arrarys\n",
        "        x.append(img)  # appending image array\n",
        "        y.append(train_class_names.index(folder))  # appending class index to the array\n",
        "\n",
        "print(\"Preparing Training Dataset Completed.\")\n",
        "\n",
        "# Training Dataset\n",
        "print(\"Training Dataset\")\n",
        "\n",
        "x = np.array(x)  # Converting to np arrary to pass to the model\n",
        "print(x.shape)\n",
        "\n",
        "y = to_categorical(y)  # onehot encoding of the labels\n",
        "# print(y)\n",
        "print(y.shape)\n"
      ],
      "metadata": {
        "id": "Aa4ywq5JJpTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTHXjyKsJmXS"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "this moudule imports and train the model using VGG16 algorithm\n",
        "\n",
        "'''\n",
        "print(\"Summary of default VGG16 model.\\n\")\n",
        "# we are using VGG16 for transfer learnin here. So we have imported it\n",
        "from tensorflow.keras.applications import VGG16\n",
        "\n",
        "model_vgg16 = VGG16(weights='imagenet')\n",
        "model_vgg16.summary()\n",
        "\n",
        "print(\"Summary of Custom VGG16 model.\\n\")\n",
        "input_layer = layers.Input(shape=(img_width, img_height, 3))\n",
        "model_vgg16 = VGG16(weights='imagenet', input_tensor=input_layer, include_top=False)\n",
        "model_vgg16.summary()\n",
        "last_layer = model_vgg16.output\n",
        "flatten = layers.Flatten()(last_layer)\n",
        "output_layer = layers.Dense(6, activation='softmax')(flatten)\n",
        "model = models.Model(inputs=input_layer, outputs=output_layer)\n",
        "\n",
        "model.summary()\n",
        "for layer in model.layers[:-1]:\n",
        "    layer.trainable = False\n",
        "model.summary()\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=0)\n",
        "print(\"Splitting data for train and test completed.\")\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "print(\"Model compilation completed.\")\n",
        "\n",
        "history2 = model.fit(xtrain, ytrain, epochs=epochs, batch_size=batch_size, verbose=True, validation_data=(xtrain, ytrain))\n",
        "\n",
        "print(\"Fitting the model completed.\")\n",
        "\n",
        "model.save(\"Vggmodel.h5\")\n",
        "\n",
        "acc = history2.history['accuracy']\n",
        "val_acc = history2.history['val_accuracy']\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, label='Training Accuracy')\n",
        "plt.plot(epochs, val_acc, label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Plot Model Loss\n",
        "loss_train = history2.history['loss']\n",
        "loss_val = history2.history['val_loss']\n",
        "plt.plot(epochs, loss_train, label='Training Loss')\n",
        "plt.plot(epochs, loss_val, label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "y_pred = model.predict(xtrain)\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "print(y_pred)\n",
        "y_test=np.argmax(ytrain, axis=1)\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix of Vgg16 ')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "this will predict the result of the image uploaded by the user whether it is a degragable or non-degragable item by precdition result\n",
        "\n",
        "'''\n",
        "from flask import Flask, render_template, flash, request, session\n",
        "import warnings\n",
        "import os\n",
        "import mysql.connector\n",
        "\n",
        "app = Flask(__name__)\n",
        "app.config.from_object(__name__)\n",
        "app.config['SECRET_KEY'] = '<add secretkey>'\n",
        "\n",
        "\n",
        "@app.route(\"/\")\n",
        "def homepage():\n",
        "    return render_template('index.html')\n",
        "\n",
        "\n",
        "@app.route(\"/Prediction\")\n",
        "def Prediction():\n",
        "    return render_template('Prediction.html')\n",
        "\n",
        "\n",
        "@app.route(\"/predict\", methods=['GET', 'POST'])\n",
        "def predict():\n",
        "    if request.method == 'POST':\n",
        "        import tensorflow as tf\n",
        "        import numpy as np\n",
        "        import cv2\n",
        "        from keras.preprocessing import image\n",
        "        file = request.files['file']\n",
        "        file.save('static/upload/Test.jpg')\n",
        "        org = 'static/upload/Test.jpg'\n",
        "\n",
        "        img1 = cv2.imread('static/upload/Test.jpg')\n",
        "\n",
        "        dst = cv2.fastNlMeansDenoisingColored(img1, None, 10, 10, 7, 21)\n",
        "        noi = 'static/upload/noi.jpg'\n",
        "        cv2.imwrite(noi, dst)\n",
        "\n",
        "        classifierLoad = tf.keras.models.load_model('Vggmodel.h5')\n",
        "        test_image = image.load_img('static/upload/Test.jpg', target_size=(100, 100))\n",
        "        test_image = np.expand_dims(test_image, axis=0)\n",
        "        result = classifierLoad.predict(test_image)\n",
        "        print(result)\n",
        "\n",
        "        result = classifierLoad.predict(test_image)\n",
        "        print(result)\n",
        "        result = np.argmax(result, axis=1)\n",
        "\n",
        "        print(result)\n",
        "\n",
        "        out = ''\n",
        "        pre = ''\n",
        "        if result[0] == 0:\n",
        "            print(\"cardboard\")\n",
        "            out = \"cardboard\"\n",
        "            pre = \"Degradable\"\n",
        "        elif result[0] == 1:\n",
        "            print(\"glass\")\n",
        "            out = \"glass\"\n",
        "            pre = \"Non-Degradable\"\n",
        "        elif result[0] == 2:\n",
        "            print(\"metal\")\n",
        "            out = \"metal\"\n",
        "            pre = \"Non-Degradable\"\n",
        "        elif result[0] == 3:\n",
        "            print(\"paper\")\n",
        "            out = \"paper\"\n",
        "            pre = \"Degradable\"\n",
        "\n",
        "        elif result[0] == 4:\n",
        "            print(\"plastic\")\n",
        "            out = \"plastic\"\n",
        "            pre = \"Non-Degradable\"\n",
        "        elif result[0] == 5:\n",
        "            print(\"trash\")\n",
        "            out = \"trash\"\n",
        "            pre = \"Degradable\"\n",
        "\n",
        "        else:\n",
        "            print(\"Invalid\")\n",
        "\n",
        "        return render_template('Result.html', res=out, pre=pre, org=org, noi=noi)\n",
        "\n"
      ],
      "metadata": {
        "id": "bqviyJtbKHHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "this module will connect the camera in the device for the user input and give the prediction result to the user\n",
        "\n",
        "'''\n",
        "@app.route(\"/Camera\")\n",
        "def Camera():\n",
        "    import warnings\n",
        "    import cv2\n",
        "    import numpy as np\n",
        "    import os\n",
        "    import time\n",
        "    args = {\"confidence\": 0.5, \"threshold\": 0.3}\n",
        "    flag = False\n",
        "\n",
        "    labelsPath = \"./yolo-coco/coco.names\"\n",
        "    LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
        "    final_classes = ['bottle', 'wine glass', 'cup', 'cell phone', 'book']\n",
        "\n",
        "    np.random.seed(42)\n",
        "    COLORS = np.random.randint(0, 255, size=(len(LABELS), 3),\n",
        "                               dtype=\"uint8\")\n",
        "\n",
        "    weightsPath = os.path.abspath(\"./yolo-coco/yolov3-tiny.weights\")\n",
        "    configPath = os.path.abspath(\"./yolo-coco/yolov3-tiny.cfg\")\n",
        "\n",
        "    # print(configPath, \"\\n\", weightsPath)\n",
        "\n",
        "    net = cv2.dnn.readNetFromDarknet(configPath, weightsPath)\n",
        "    ln = net.getLayerNames()\n",
        "    ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
        "\n",
        "    vs = cv2.VideoCapture(0)\n",
        "    writer = None\n",
        "    (W, H) = (None, None)\n",
        "\n",
        "    flag = True\n",
        "\n",
        "    flagg = 0\n",
        "\n",
        "    while True:\n",
        "        # read the next frame from the file\n",
        "        (grabbed, frame) = vs.read()\n",
        "\n",
        "        # if the frame was not grabbed, then we have reached the end\n",
        "        # of the stream\n",
        "        if not grabbed:\n",
        "            break\n",
        "\n",
        "        # if the frame dimensions are empty, grab them\n",
        "        if W is None or H is None:\n",
        "            (H, W) = frame.shape[:2]\n",
        "\n",
        "        blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416),\n",
        "                                     swapRB=True, crop=False)\n",
        "        net.setInput(blob)\n",
        "        start = time.time()\n",
        "        layerOutputs = net.forward(ln)\n",
        "        end = time.time()\n",
        "\n",
        "        # initialize our lists of detected bounding boxes, confidences,\n",
        "        # and class IDs, respectively\n",
        "        boxes = []\n",
        "        confidences = []\n",
        "        classIDs = []\n",
        "\n",
        "        # loop over each of the layer outputs\n",
        "        for output in layerOutputs:\n",
        "            # loop over each of the detections\n",
        "            for detection in output:\n",
        "                # extract the class ID and confidence (i.e., probability)\n",
        "                # of the current object detection\n",
        "                scores = detection[5:]\n",
        "                classID = np.argmax(scores)\n",
        "                confidence = scores[classID]\n",
        "\n",
        "                # filter out weak predictions by ensuring the detected\n",
        "                # probability is greater than the minimum probability\n",
        "                if confidence > args[\"confidence\"]:\n",
        "                    # scale the bounding box coordinates back relative to\n",
        "                    # the size of the image, keeping in mind that YOLO\n",
        "                    # actually returns the center (x, y)-coordinates of\n",
        "                    # the bounding box followed by the boxes' width and\n",
        "                    # height\n",
        "                    box = detection[0:4] * np.array([W, H, W, H])\n",
        "                    (centerX, centerY, width, height) = box.astype(\"int\")\n",
        "\n",
        "                    # use the center (x, y)-coordinates to derive the top\n",
        "                    # and and left corner of the bounding box\n",
        "                    x = int(centerX - (width / 2))\n",
        "                    y = int(centerY - (height / 2))\n",
        "\n",
        "                    # update our list of bounding box coordinates,\n",
        "                    # confidences, and class IDs\n",
        "                    boxes.append([x, y, int(width), int(height)])\n",
        "                    confidences.append(float(confidence))\n",
        "                    classIDs.append(classID)\n",
        "        # apply non-maxima suppression to suppress weak, overlapping\n",
        "        # bounding boxes\n",
        "        idxs = cv2.dnn.NMSBoxes(boxes, confidences, args[\"confidence\"],\n",
        "                                args[\"threshold\"])\n",
        "\n",
        "        # ensure at least one detection exists\n",
        "        if len(idxs) > 0:\n",
        "            # loop over the indexes we are keeping\n",
        "            for i in idxs.flatten():\n",
        "                # extract the bounding box coordinates\n",
        "                (x, y) = (boxes[i][0], boxes[i][1])\n",
        "                (w, h) = (boxes[i][2], boxes[i][3])\n",
        "\n",
        "                if (LABELS[classIDs[i]] in final_classes):\n",
        "\n",
        "                    flagg += 1\n",
        "                    # print(flag)\n",
        "                    # sendmsg(\"9486365535\", \" Animal Detected \" + LABELS[classIDs[i]])\n",
        "\n",
        "                    color = [int(c) for c in COLORS[classIDs[i]]]\n",
        "                    cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
        "                    text = \"{}: {:.4f}\".format(LABELS[classIDs[i]],\n",
        "                                               confidences[i])\n",
        "                    cv2.putText(frame, text, (x, y - 5),\n",
        "                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "                    if (flagg == 40):\n",
        "                        flagg = 0\n",
        "\n",
        "                        out = LABELS[classIDs[i]]\n",
        "                        pre = ''\n",
        "                        if out==\"bottle\":\n",
        "                            pre = \"Non-Degradable\"\n",
        "                        elif out==\"wine glass\":\n",
        "                            pre = \"Non-Degradable\"\n",
        "                        elif out==\"cup\":\n",
        "                            pre = \"Degradable\"\n",
        "                        elif out == \"cell phone\":\n",
        "                            pre = \"Non-Degradable\"\n",
        "                        elif out == \"book\":\n",
        "                            pre = \"Degradable\"\n",
        "                        sendmsg(\"9486365535\",'Prediction Result : ' + out + ' ' + pre)\n",
        "        else:\n",
        "            flag = True\n",
        "        cv2.imshow(\"Output\", frame)\n",
        "        if cv2.waitKey(1) == ord('q'):\n",
        "            break\n",
        "    # release the webcam and destroy all active windows\n",
        "    vs.release()\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "    return render_template('index.html')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # app.run(host='0.0.0.0',debug = True, port = 5000)\n",
        "    app.run(debug=True, use_reloader=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "L1Ufj-5lKTqv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}